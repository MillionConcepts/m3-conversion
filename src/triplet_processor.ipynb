{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# notebook for processing fully reduced m3 data \"triplets\"\n",
    "This is a notebook for processing L0 / L1B / L2 triplets (i.e.,\n",
    "the observations that got reduced).\n",
    "\n",
    "## general notes\n",
    "\n",
    "We process the reduced data in triplets simply to improve the metadata on the\n",
    "L0 and L2 products. We convert L1B first to extract several attributes to fill\n",
    "out their metadata. This data is scratched to disk in\n",
    "[./directories/m3/m3_index.csv'](./directories/m3/m3_index.csv), because it\n",
    "also serves as a useful user-facing index to the archive. A complete version\n",
    "of this index is provided in this repository, but this index was originally\n",
    "created during this conversion process, and will be recreated if you run it\n",
    "again. This index is read into the ```m3_index variable``` below; its path is\n",
    "also soft-coded in several ```m3_conversion``` classes, so make sure you\n",
    "change that or feed them the correct path as an argument if you change this\n",
    "location.\n",
    "\n",
    "This notebook does not apply programmatic rules to iterate over the file\n",
    "structure of the mirrored archive. It uses an index that was partly manually\n",
    "generated:\n",
    "[/src/directories/m3/m3_data_mappings.csv](/src/directories/m3/m3_data_mappings.csv).\n",
    "This was manually manipulated to manage several small idiosyncracies in the\n",
    "PDS3 archive.\n",
    "\n",
    "35 of the V3 L1B products in the PDS3 archive are duplicated: one copy in the\n",
    "correct month-by-year directory, one copy in some incorrect month-by-year\n",
    "directory. We pick the 'first' one in all cases (see the line\n",
    "```pds3_label_file = input_directory + group_files[product_type][0]``` below).\n",
    "Each pair's members have identical md5sums, so it *probably* doesn't matter\n",
    "which member of the pair we use.\n",
    "\n",
    "## performance tips\n",
    "\n",
    "The most likely bottlenecks for this process are I/O throughput and CPU. We\n",
    "recommend both using a high-throughput disk and parallelizing this, either\n",
    "using ```pathos``` (vanilla Python ```multiprocessing``` will probably fail\n",
    "during a pickling step involving ```rasterio```) or simply by running multiple\n",
    "copies of this notebook. If you do parallelize this process on a single\n",
    "machine, note that working memory can suddenly catch you off-guard as a\n",
    "constraint. While many of the M3 observational data files are small, some are\n",
    "over 4 GB, and the method presented here requires them to be completely loaded\n",
    "into memory in order to convert them to FITS and strip the prefix tables from\n",
    "the L0 files. When passed ```clean=True```, the ```m3_converter```\n",
    "observational data writer class constructors aggressively delete data after\n",
    "using it, but this still results in a pretty high -- and spiky -- working\n",
    "memory burden."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "from types import MappingProxyType\n",
    "\n",
    "from more_itertools import distribute\n",
    "import pandas as pd\n",
    "import sh\n",
    "\n",
    "from m3_bulk import basenamer, make_m3_triplet, \\\n",
    "    m3_triplet_bundle_paths, crude_time_log, fix_end_object_tags\n",
    "from m3_conversion import M3L0Converter, M3L1BConverter, M3L2Converter\n",
    "from pvl.decoder import ParseError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m3_index = pd.read_csv('./directories/m3/m3_index.csv')\n",
    "\n",
    "# directory of file mappings, grouped into m3 basename clusters\n",
    "file_mappings = pd.read_csv('./directories/m3/m3_data_mappings.csv')\n",
    "file_mappings[\"basename\"] = file_mappings[\"filepath\"].apply(basenamer)\n",
    "basename_groups = list(file_mappings.groupby(\"basename\"))\n",
    "\n",
    "# what kind of files does each pds4 product have?\n",
    "# paths to the locally-written versions are stored in the relevant attributes of \n",
    "# the associated PDSVersionConverter instance.\n",
    "pds4_filetypes = MappingProxyType({\n",
    "    'l0': ('pds4_label_file', 'clock_file', 'fits_image_file'),\n",
    "    'l1b': ('pds4_label_file', 'loc_file', 'tim_file', 'rdn_file', 'obs_file'),\n",
    "    'l2': ('pds4_label_file', 'sup_file', 'rfl_file')\n",
    "})\n",
    "\n",
    "# root directories of PDS3 and PDS4 data sets respectively\n",
    "input_directory = '/home/ubuntu/m3_input/'\n",
    "output_directory = '/home/ubuntu/m3_output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# all the triplets: what we are converting here.\n",
    "reduced_groups = [group for group in basename_groups if len(group[1]) >= 3]\n",
    "\n",
    "# the\n",
    "edr_groups = [group for group in basename_groups if len(group[1]) == 1] # lonesome EDR images\n",
    "\n",
    "triplet_product_types = ('l1b', 'l0', 'l2')\n",
    "\n",
    "# initialize our mapping of product types to\n",
    "# product-writer class constructors.\n",
    "# MappingProxyType is just a safety mechanism\n",
    "# to make sure constructors don't get messed with\n",
    "converters = MappingProxyType({\n",
    "    'l0': M3L0Converter,\n",
    "    'l1b': M3L1BConverter,\n",
    "    'l2': M3L2Converter\n",
    "})\n",
    "writers = {}  # dict to hold instances of the converter classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize iteration, control execution in whatever way\n",
    "\n",
    "# this is a place to split your index up however you like\n",
    "# if you're parallelizing using multiple copies of this\n",
    "# notebook.\n",
    "chunk_ix_of_this_notebook = 0\n",
    "total_chunks = 40\n",
    "chunks = distribute(total_chunks, reduced_groups)\n",
    "# eagerly evaluate so we know how long it is,\n",
    "# and what all is in it if we have an error\n",
    "chunk = list(chunks[chunk_ix_of_this_notebook])\n",
    "log_string = \"_\" + str(chunk_ix_of_this_notebook)\n",
    "\n",
    "group_enumerator = enumerate(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ix, group in group_enumerator:\n",
    "\n",
    "    print(ix, len(chunk))\n",
    "    print(\"beginning product conversion\")\n",
    "    triplet_start_time = dt.datetime.now()\n",
    "    group_files = make_m3_triplet(group)\n",
    "    # what are the correct output paths (relative to\n",
    "    # the root of the pds4 bundle) for these products?\n",
    "    bundle_paths = m3_triplet_bundle_paths(group)\n",
    "\n",
    "    for product_type in triplet_product_types:\n",
    "        # read the PDS3 product and perform file conversions \n",
    "        pds3_label_file = input_directory + group_files[product_type][0]\n",
    "        try: \n",
    "            writers[product_type] = converters[product_type](\n",
    "                pds3_label_file, suppress_warnings=True, clean=True\n",
    "            )\n",
    "        except ParseError: # fix broken END_OBJECT tags in some of the target-mode files  \n",
    "            print(\"fixing broken END_OBJECT tags\")\n",
    "            temp_label_file = fix_end_object_tags(pds3_label_file)\n",
    "            writers[product_type] = converters[product_type](\n",
    "                temp_label_file, suppress_warnings=True, clean=True\n",
    "            )\n",
    "            os.remove(temp_label_file)\n",
    "        # write PDS4 label and product files\n",
    "        # don't actually need to shave the extra / here but...\n",
    "        # this would be more safely rewritten with PyFilesystem\n",
    "        # (see clem-conversion)\n",
    "        output_path = output_directory + bundle_paths[product_type][1:] \n",
    "        sh.mkdir(\"-p\", output_path)\n",
    "        writers[product_type].write_pds4(output_path, write_product_files=True, clean=True)    \n",
    "\n",
    "        # occasionally (slow but very useful) spot-check with validate tool\n",
    "        # note that this just invokes a one-line script at /usr/bin/validate\n",
    "        # that links to the local install of the PDS Validate Tool; this\n",
    "        # allows us to avoid throwing java stuff all over our environment\n",
    "        if ix % 20 == 1:\n",
    "            print(\"1-mod-20th triplet: running Validate Tool\")\n",
    "            validate_results = sh.validate(\"-t\", writers[product_type].pds4_label_file)\n",
    "            with open(\"validate_dump.txt\", \"a\") as file:\n",
    "                file.write(validate_results.stdout.decode())\n",
    "            print(\"validated successfully\")\n",
    "        # log transfer crudely\n",
    "        crude_time_log(\n",
    "            \"m3_data_conversion_log\" + log_string + \".csv\",\n",
    "            writers[product_type], \n",
    "            str((dt.datetime.now() - triplet_start_time).total_seconds())\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"done with this triplet; total seconds \" \n",
    "        + str((dt.datetime.now() - triplet_start_time).total_seconds())\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}